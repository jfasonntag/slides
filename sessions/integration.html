<?xml version="1.0" encoding="utf-8"?>
<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Strict//EN"
 "http://www.w3.org/TR/xhtml1/DTD/xhtml1-strict.dtd">
<html xmlns="http://www.w3.org/1999/xhtml">
<head>
  <meta http-equiv="Content-Type" content="text/html; charset=utf-8" />
  <meta http-equiv="Content-Style-Type" content="text/css" />
  <meta name="generator" content="pandoc" />
  <meta name="author" content="Florian Oswald" />
  <title>Computational Economics: Numerical Integrals and Derivatives</title>
  <style type="text/css">code{white-space: pre;}</style>
  <style type="text/css">
div.sourceCode { overflow-x: auto; }
table.sourceCode, tr.sourceCode, td.lineNumbers, td.sourceCode {
  margin: 0; padding: 0; vertical-align: baseline; border: none; }
table.sourceCode { width: 100%; line-height: 100%; }
td.lineNumbers { text-align: right; padding-right: 4px; padding-left: 4px; background-color: #dddddd; }
td.sourceCode { padding-left: 5px; }
code > span.kw { font-weight: bold; } /* Keyword */
code > span.dt { color: #800000; } /* DataType */
code > span.dv { color: #0000ff; } /* DecVal */
code > span.bn { color: #0000ff; } /* BaseN */
code > span.fl { color: #800080; } /* Float */
code > span.ch { color: #ff00ff; } /* Char */
code > span.st { color: #dd0000; } /* String */
code > span.co { color: #808080; font-style: italic; } /* Comment */
code > span.al { color: #00ff00; font-weight: bold; } /* Alert */
code > span.fu { color: #000080; } /* Function */
code > span.er { color: #ff0000; font-weight: bold; } /* Error */
code > span.wa { color: #ff0000; font-weight: bold; } /* Warning */
code > span.cn { color: #000000; } /* Constant */
code > span.sc { color: #ff00ff; } /* SpecialChar */
code > span.vs { color: #dd0000; } /* VerbatimString */
code > span.ss { color: #dd0000; } /* SpecialString */
code > span.im { } /* Import */
code > span.va { } /* Variable */
code > span.cf { } /* ControlFlow */
code > span.op { } /* Operator */
code > span.bu { } /* BuiltIn */
code > span.ex { } /* Extension */
code > span.pp { font-weight: bold; } /* Preprocessor */
code > span.at { } /* Attribute */
code > span.do { color: #808080; font-style: italic; } /* Documentation */
code > span.an { color: #808080; font-weight: bold; font-style: italic; } /* Annotation */
code > span.cv { color: #808080; font-weight: bold; font-style: italic; } /* CommentVar */
code > span.in { color: #808080; font-weight: bold; font-style: italic; } /* Information */
  </style>
  <link rel="stylesheet" type="text/css" media="screen, projection, print"
    href="http://www.w3.org/Talks/Tools/Slidy2/styles/slidy.css" />
  <script src="http://www.w3.org/Talks/Tools/Slidy2/scripts/slidy.js"
    charset="utf-8" type="text/javascript"></script>
</head>
<body>
<div class="slide titlepage">
  <h1 class="title">Computational Economics: Numerical Integrals and Derivatives</h1>
  <p class="author">
Florian Oswald
  </p>
  <p class="date">Sciences Po, 2017</p>
</div>
<div id="numerical-differentiation-and-integration" class="slide section level1">
<h1>Numerical Differentiation and Integration</h1>
<h2 id="derivatives">Derivatives</h2>
<ol style="list-style-type: decimal">
<li>Finite Differencing: a numerical approximation
<ul>
<li>Based on Taylor's Theorem</li>
<li>Observe variation in function values from evaluating it at &quot;close&quot; points.</li>
<li>Forward Differencing and Central Differencing</li>
</ul></li>
<li>Automatic Differentiation
<ul>
<li>Breaks down the actual <code>code</code> that defines a function and performs elementary differentiation rules, after disecting expressions via the chain rule.</li>
<li>This produces <strong>analytic</strong> derivatives, i.e. there is <strong>no</strong> approximation error.</li>
<li>This is the future.</li>
</ul></li>
<li>Symbolic Differentiation
<ul>
<li>Some languages (most notably Mathematica) support symbolic algebra. Very useful sometimes if one needs to work through complicated expressions.</li>
<li>Not very useful for high computational demands, i.e. repeated computation of derivatives in an optimization routine.</li>
</ul></li>
</ol>
</div>
<div id="finite-differences" class="slide section level1">
<h1>Finite Differences</h1>
<ul>
<li>Consider the definition of the derivative of <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>f</mi><annotation encoding="application/x-tex">f</annotation></semantics></math> at point <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>x</mi><annotation encoding="application/x-tex">x</annotation></semantics></math>: <math display="block" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>f</mi><mi>‚Ä≤</mi><mo stretchy="false" form="prefix">(</mo><mi>x</mi><mo stretchy="false" form="postfix">)</mo><mo>=</mo><munder><mo>lim</mo><mrow><mi>h</mi><mo accent="false">‚Üí</mo><mn>0</mn></mrow></munder><mfrac><mrow><mi>f</mi><mo stretchy="false" form="prefix">(</mo><mi>x</mi><mo>+</mo><mi>h</mi><mo stretchy="false" form="postfix">)</mo><mo>‚àí</mo><mi>f</mi><mo stretchy="false" form="prefix">(</mo><mi>x</mi><mo stretchy="false" form="postfix">)</mo></mrow><mi>h</mi></mfrac></mrow><annotation encoding="application/x-tex"> f&#39;(x) = \lim_{h\to0}\frac{f(x+h)-f(x)}{h} </annotation></semantics></math></li>
<li>The simplest way to calculate a numerical derivative is to replicate this computation for small <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>h</mi><annotation encoding="application/x-tex">h</annotation></semantics></math> with: <math display="block" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>f</mi><mi>‚Ä≤</mi><mo stretchy="false" form="prefix">(</mo><mi>x</mi><mo stretchy="false" form="postfix">)</mo><mo>‚âà</mo><mfrac><mrow><mi>f</mi><mo stretchy="false" form="prefix">(</mo><mi>x</mi><mo>+</mo><mi>h</mi><mo stretchy="false" form="postfix">)</mo><mo>‚àí</mo><mi>f</mi><mo stretchy="false" form="prefix">(</mo><mi>x</mi><mo stretchy="false" form="postfix">)</mo></mrow><mi>h</mi></mfrac><mo>,</mo><mspace width="1.0em"></mspace><mi>h</mi><mrow><mspace width="0.333em"></mspace><mtext mathvariant="normal"> small.</mtext></mrow></mrow><annotation encoding="application/x-tex"> f&#39;(x) \approx \frac{f(x+h)-f(x)}{h},\quad h\text{ small.} </annotation></semantics></math></li>
<li>This is known as the Forward Difference approach.</li>
<li>There are different approaches, e.g. the central difference approach does <math display="block" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>f</mi><mi>‚Ä≤</mi><mo stretchy="false" form="prefix">(</mo><mi>x</mi><mo stretchy="false" form="postfix">)</mo><mo>‚âà</mo><mfrac><mrow><mi>f</mi><mo stretchy="false" form="prefix">(</mo><mi>x</mi><mo>+</mo><mi>h</mi><mo stretchy="false" form="postfix">)</mo><mo>‚àí</mo><mi>f</mi><mo stretchy="false" form="prefix">(</mo><mi>x</mi><mo>‚àí</mo><mi>h</mi><mo stretchy="false" form="postfix">)</mo></mrow><mrow><mn>2</mn><mi>h</mi></mrow></mfrac><mo>,</mo><mspace width="1.0em"></mspace><mi>h</mi><mrow><mspace width="0.333em"></mspace><mtext mathvariant="normal"> small.</mtext></mrow></mrow><annotation encoding="application/x-tex"> f&#39;(x) \approx \frac{f(x+h)-f(x-h)}{2h},\quad h\text{ small.} </annotation></semantics></math></li>
<li>How does this perform?</li>
</ul>
<div class="sourceCode"><pre class="sourceCode julia"><code class="sourceCode julia">using Gadfly
f(x) = <span class="fl">2</span> - x^<span class="fl">2</span>
c = -<span class="fl">0.75</span>
sec_line(h) = x -&gt; f(c) + (f(c + h) - f(c))/h * (x - c)
plot([f, sec_line(<span class="fl">1</span>), sec_line(<span class="fl">.5</span>), sec_line(<span class="fl">.25</span>), sec_line(<span class="fl">.05</span>)], -<span class="fl">1</span>, <span class="fl">1</span>)</code></pre></div>
<ul>
<li>What's the problem? Well, what is <em>small</em>?</li>
</ul>
</div>
<div id="finite-differences-whats-the-right-step-size-h" class="slide section level1">
<h1>Finite Differences: what's the right step size <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>h</mi><annotation encoding="application/x-tex">h</annotation></semantics></math>?</h1>
<ul>
<li>Theoretically, we would like to have <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>h</mi><annotation encoding="application/x-tex">h</annotation></semantics></math> as small as possible, since we want to approximate the limit at zero.</li>
<li>In practice, on a computer, there is a limit to this. There is a smallest representable number, as we know.</li>
<li><code>eps()</code>.</li>
<li>One can show that the optimal step size is <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>h</mi><mo>=</mo><msqrt><mtext mathvariant="monospace">ùöéùöôùöú()</mtext></msqrt></mrow><annotation encoding="application/x-tex">h=\sqrt{\texttt{eps()}}</annotation></semantics></math></li>
</ul>
</div>
<div id="automatic-differentiation-ad" class="slide section level1">
<h1>¬†Automatic Differentiation (AD)</h1>
<ul>
<li>2 modes: Forward and Reverse Mode.</li>
<li>The basic idea is that the derivative of any function can be decomposed into some basic algebraic operations.</li>
<li><p>The <a href="https://en.wikipedia.org/wiki/Automatic_differentiation">wikipedia page is informative</a></p>
<div class="figure">
<img src="figs/wikipedia-AD.png" alt="By Berland at en.wikipedia [Public domain], from Wikimedia Commons" />
<p class="caption"><a href="https://commons.wikimedia.org/wiki/File%3AAutomaticDifferentiation.png">By Berland at en.wikipedia [Public domain], from Wikimedia Commons</a></p>
</div></li>
</ul>
<h2 id="example">Example</h2>
<ul>
<li>Suppose we want to differentiate <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>f</mi><mo stretchy="false" form="prefix">(</mo><msub><mi>x</mi><mn>1</mn></msub><mo>,</mo><msub><mi>x</mi><mn>2</mn></msub><mo stretchy="false" form="postfix">)</mo><mo>=</mo><msub><mi>x</mi><mn>1</mn></msub><msub><mi>x</mi><mn>2</mn></msub><mo>+</mo><mo>sin</mo><msub><mi>x</mi><mn>1</mn></msub></mrow><annotation encoding="application/x-tex">f(x_1,x_2) = x_1 x_2 + \sin x_1</annotation></semantics></math></li>
<li>We label subexpressions by <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><msub><mi>w</mi><mi>i</mi></msub><annotation encoding="application/x-tex">w_i</annotation></semantics></math> as follows: <math display="block" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mtable><mtr><mtd columnalign="center"><mi>f</mi><mo stretchy="false" form="prefix">(</mo><msub><mi>x</mi><mn>1</mn></msub><mo>,</mo><msub><mi>x</mi><mn>2</mn></msub><mo stretchy="false" form="postfix">)</mo></mtd><mtd columnalign="left"><mo>=</mo><msub><mi>x</mi><mn>1</mn></msub><msub><mi>x</mi><mn>2</mn></msub><mo>+</mo><mo>sin</mo><msub><mi>x</mi><mn>1</mn></msub></mtd></mtr><mtr><mtd columnalign="center"></mtd><mtd columnalign="left"><mo>=</mo><msub><mi>w</mi><mn>1</mn></msub><msub><mi>w</mi><mn>2</mn></msub><mo>+</mo><mi>s</mi><mi>i</mi><mi>n</mi><msub><mi>w</mi><mn>1</mn></msub></mtd></mtr><mtr><mtd columnalign="center"></mtd><mtd columnalign="left"><mo>=</mo><msub><mi>w</mi><mn>3</mn></msub><mo>+</mo><msub><mi>w</mi><mn>4</mn></msub></mtd></mtr><mtr><mtd columnalign="center"></mtd><mtd columnalign="left"><mo>=</mo><msub><mi>w</mi><mn>5</mn></msub></mtd></mtr></mtable><annotation encoding="application/x-tex"> \begin{array}{cl}
f(x_1,x_2) &amp;= x_1 x_2 + \sin x_1 \\
&amp;= w_1 w_2 + sin w_1 \\
&amp;= w_3  + w_4 \\
&amp;= w_5 
\end{array} 
</annotation></semantics></math></li>
<li>Computation of the partial derivative starts with the seed value, i.e. <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mover><mi>w</mi><mo accent="true">Ãá</mo></mover><mn>1</mn></msub><mo>=</mo><mfrac><mrow><mi>‚àÇ</mi><msub><mi>x</mi><mn>1</mn></msub></mrow><mrow><mi>‚àÇ</mi><msub><mi>x</mi><mn>1</mn></msub></mrow></mfrac><mo>=</mo><mn>1</mn></mrow><annotation encoding="application/x-tex">\dot{w}_1 = \frac{\partial x_1}{\partial x_1} = 1</annotation></semantics></math>.</li>
<li>We store for each subexpression both the value and the derivative, i.e. <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mo stretchy="false" form="prefix">(</mo><msub><mi>w</mi><mi>i</mi></msub><mo>,</mo><msub><mover><mi>w</mi><mo accent="true">Ãá</mo></mover><mi>i</mi></msub><mo stretchy="false" form="postfix">)</mo></mrow><annotation encoding="application/x-tex">(w_i,\dot{w}_i)</annotation></semantics></math></li>
<li><p>We then sweep through the expression tree as in this picture:</p>
<div class="figure">
<img src="figs/wikipedia-AD-example.png" alt="By Berland at en.wikipedia [Public domain], from Wikimedia Commons" />
<p class="caption"><a href="https://commons.wikimedia.org/wiki/File%3AForwardAccumulationAutomaticDifferentiation.png">By Berland at en.wikipedia [Public domain], from Wikimedia Commons</a></p>
</div></li>
</ul>
</div>
<div id="ad-in-julia" class="slide section level1">
<h1>AD in Julia</h1>
<ul>
<li>The organisation here is <a href="http://www.juliadiff.org" class="uri">http://www.juliadiff.org</a></li>
<li>There are many packages to perform differentiation with Julia here.</li>
<li>Many packages rely on the machinery here.</li>
<li>Let's quickly look at <a href="https://github.com/JuliaDiff/ForwardDiff.jl" class="uri">https://github.com/JuliaDiff/ForwardDiff.jl</a></li>
</ul>
<div class="sourceCode"><pre class="sourceCode julia"><code class="sourceCode julia"><span class="co"># from ForwardDiff&#39;s readme:</span>
using ForwardDiff
f(x::<span class="dt">Vector</span>) = sum(sin, x) + prod(tan, x) * sum(sqrt, x)
x = rand(<span class="fl">5</span>); <span class="co"># get 5 random points</span>
g = ForwardDiff.gradient(f); <span class="co"># g = ‚àáf</span>
j = ForwardDiff.jacobian(g); <span class="co"># j = J(‚àáf)</span>
ForwardDiff.hessian(f, x) <span class="co"># H(f)(x) == J(‚àáf)(x), as expected</span></code></pre></div>
<ul>
<li>The authors provide some benchmarks. Let's run those:</li>
</ul>
<div class="sourceCode"><pre class="sourceCode julia"><code class="sourceCode julia">include(joinpath(Pkg.dir(<span class="st">&quot;ForwardDiff&quot;</span>),<span class="st">&quot;benchmarks&quot;</span>,<span class="st">&quot;run_all_benchmarks(ForwardDiffBenchmarks.Rosenbrock)&quot;</span>))</code></pre></div>
</div>
<div id="numerical-approximation-of-integrals" class="slide section level1">
<h1>Numerical Approximation of Integrals</h1>
<ul>
<li>We will focus on methods that represent integrals as weighted sums.</li>
<li>The typical representation will look like: <math display="block" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>E</mi><mo stretchy="false" form="prefix">[</mo><mi>G</mi><mo stretchy="false" form="prefix">(</mo><mi>œµ</mi><mo stretchy="false" form="postfix">)</mo><mo stretchy="false" form="postfix">]</mo><mo>=</mo><msub><mo>‚à´</mo><msup><mstyle mathvariant="double-struck"><mi>‚Ñù</mi></mstyle><mi>N</mi></msup></msub><mi>G</mi><mo stretchy="false" form="prefix">(</mo><mi>œµ</mi><mo stretchy="false" form="postfix">)</mo><mi>w</mi><mo stretchy="false" form="prefix">(</mo><mi>œµ</mi><mo stretchy="false" form="postfix">)</mo><mi>d</mi><mi>œµ</mi><mo>‚âà</mo><munderover><mo>‚àë</mo><mrow><mi>j</mi><mo>=</mo><mn>1</mn></mrow><mi>J</mi></munderover><msub><mi>œâ</mi><mi>j</mi></msub><mi>G</mi><mo stretchy="false" form="prefix">(</mo><msub><mi>œµ</mi><mi>j</mi></msub><mo stretchy="false" form="postfix">)</mo></mrow><annotation encoding="application/x-tex"> E[G(\epsilon)] = \int_{\mathbb{R}^N} G(\epsilon) w(\epsilon) d\epsilon \approx \sum_{j=1}^J \omega_j G(\epsilon_j) </annotation></semantics></math>
<ul>
<li><math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>N</mi><annotation encoding="application/x-tex">N</annotation></semantics></math> is the dimensionality of the integration problem.</li>
<li><math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>G</mi><mo>:</mo><msup><mstyle mathvariant="double-struck"><mi>‚Ñù</mi></mstyle><mi>N</mi></msup><mo accent="false">‚Ü¶</mo><mstyle mathvariant="double-struck"><mi>‚Ñù</mi></mstyle></mrow><annotation encoding="application/x-tex">G:\mathbb{R}^N \mapsto \mathbb{R}</annotation></semantics></math> is the function we want to integrate wrt <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>œµ</mi><mo>‚àà</mo><msup><mstyle mathvariant="double-struck"><mi>‚Ñù</mi></mstyle><mi>N</mi></msup></mrow><annotation encoding="application/x-tex">\epsilon \in \mathbb{R}^N</annotation></semantics></math>.</li>
<li><math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>w</mi><annotation encoding="application/x-tex">w</annotation></semantics></math> is a density function s.t. <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mo>‚à´</mo><msup><mstyle mathvariant="double-struck"><mi>‚Ñù</mi></mstyle><mi>n</mi></msup></msub><mi>w</mi><mo stretchy="false" form="prefix">(</mo><mi>œµ</mi><mo stretchy="false" form="postfix">)</mo><mi>d</mi><mi>œµ</mi><mo>=</mo><mn>1</mn></mrow><annotation encoding="application/x-tex">\int_{\mathbb{R}^n} w(\epsilon) d\epsilon = 1</annotation></semantics></math>.</li>
<li><math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>œâ</mi><annotation encoding="application/x-tex">\omega</annotation></semantics></math> are weights such that (most of the time) <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msubsup><mo>‚àë</mo><mrow><mi>j</mi><mo>=</mo><mn>1</mn></mrow><mi>J</mi></msubsup><msub><mi>œâ</mi><mi>j</mi></msub><mo>=</mo><mn>1</mn></mrow><annotation encoding="application/x-tex">\sum_{j=1}^J \omega_j = 1</annotation></semantics></math>. <!-- * We will look at normal shocks $\epsilon \sim N(0_N,I_N)$
* in that case, $w(\epsilon) = (2\pi)^{-N/2} \exp \left(-\frac{1}{2}\epsilon^T \epsilon \right)$
* $I_N$ is the n by n identity matrix, i.e. there is no correlation among the shocks for now.
* Other random processes will require different weighting functions, but the principle is identical.
 --></li>
</ul></li>
<li>For now, let's say that <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>N</mi><mo>=</mo><mn>1</mn></mrow><annotation encoding="application/x-tex">N=1</annotation></semantics></math></li>
</ul>
</div>
<div id="quadrature-rules" class="slide section level1">
<h1>Quadrature Rules</h1>
<ul>
<li>We focus exclusively on those and leave Simpson and Newton Cowtes formulas out.
<ul>
<li>This is because Quadrature is the method that in many situations gives highes accuracy with lowest computational cost.</li>
</ul></li>
<li>Quadrature provides a rule to compute weights <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><msub><mi>w</mi><mi>j</mi></msub><annotation encoding="application/x-tex">w_j</annotation></semantics></math> and nodes <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><msub><mi>œµ</mi><mi>j</mi></msub><annotation encoding="application/x-tex">\epsilon_j</annotation></semantics></math>.</li>
<li>There are many different quadrature rules.</li>
<li>They differ in their domain and weighting function.</li>
<li><a href="https://en.wikipedia.org/wiki/Gaussian_quadrature" class="uri">https://en.wikipedia.org/wiki/Gaussian_quadrature</a></li>
<li>In general, we can convert our function domain to a rule-specific domain with change of variables.</li>
</ul>
</div>
<div id="gauss-hermite-expectation-of-a-normally-distributed-variable" class="slide section level1">
<h1>Gauss-Hermite: Expectation of a Normally Distributed Variable</h1>
<ul>
<li>There are many different rules, all specific to a certain random process.</li>
<li>Gauss-Hermite is designed for an integral of the form <math display="block" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msubsup><mo>‚à´</mo><mrow><mo>‚àí</mo><mi>‚àû</mi></mrow><mrow><mo>+</mo><mi>‚àû</mi></mrow></msubsup><msup><mi>e</mi><mrow><mo>‚àí</mo><msup><mi>x</mi><mn>2</mn></msup></mrow></msup><mi>G</mi><mo stretchy="false" form="prefix">(</mo><mi>x</mi><mo stretchy="false" form="postfix">)</mo><mi>d</mi><mi>x</mi></mrow><annotation encoding="application/x-tex"> \int_{-\infty}^{+\infty} e^{-x^2} G(x) dx </annotation></semantics></math> and where we would approximate <math display="block" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msubsup><mo>‚à´</mo><mrow><mo>‚àí</mo><mi>‚àû</mi></mrow><mrow><mo>+</mo><mi>‚àû</mi></mrow></msubsup><msup><mi>e</mi><mrow><mo>‚àí</mo><msup><mi>x</mi><mn>2</mn></msup></mrow></msup><mi>f</mi><mo stretchy="false" form="prefix">(</mo><mi>x</mi><mo stretchy="false" form="postfix">)</mo><mi>d</mi><mi>x</mi><mo>‚âà</mo><munderover><mo>‚àë</mo><mrow><mi>i</mi><mo>=</mo><mn>1</mn></mrow><mi>n</mi></munderover><msub><mi>œâ</mi><mi>i</mi></msub><mi>G</mi><mo stretchy="false" form="prefix">(</mo><msub><mi>x</mi><mi>i</mi></msub><mo stretchy="false" form="postfix">)</mo></mrow><annotation encoding="application/x-tex"> \int_{-\infty}^{+\infty} e^{-x^2} f(x) dx \approx \sum_{i=1}^n \omega_i G(x_i) </annotation></semantics></math></li>
<li>Now, let's say we want to approximate the expected value of function <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>f</mi><annotation encoding="application/x-tex">f</annotation></semantics></math> when it's argument <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>z</mi><mo>‚àº</mo><mi>N</mi><mo stretchy="false" form="prefix">(</mo><mi>Œº</mi><mo>,</mo><msup><mi>œÉ</mi><mn>2</mn></msup><mo stretchy="false" form="postfix">)</mo></mrow><annotation encoding="application/x-tex">z\sim N(\mu,\sigma^2)</annotation></semantics></math>: <math display="block" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>E</mi><mo stretchy="false" form="prefix">[</mo><mi>f</mi><mo stretchy="false" form="prefix">(</mo><mi>z</mi><mo stretchy="false" form="postfix">)</mo><mo stretchy="false" form="postfix">]</mo><mo>=</mo><msubsup><mo>‚à´</mo><mrow><mo>‚àí</mo><mi>‚àû</mi></mrow><mrow><mo>+</mo><mi>‚àû</mi></mrow></msubsup><mfrac><mn>1</mn><mrow><mi>œÉ</mi><msqrt><mrow><mn>2</mn><mi>œÄ</mi></mrow></msqrt></mrow></mfrac><mo>exp</mo><mrow><mo stretchy="true" form="prefix">(</mo><mo>‚àí</mo><mfrac><mrow><mo stretchy="false" form="prefix">(</mo><mi>z</mi><mo>‚àí</mo><mi>Œº</mi><msup><mo stretchy="false" form="postfix">)</mo><mn>2</mn></msup></mrow><mrow><mn>2</mn><msup><mi>œÉ</mi><mn>2</mn></msup></mrow></mfrac><mo stretchy="true" form="postfix">)</mo></mrow><mi>f</mi><mo stretchy="false" form="prefix">(</mo><mi>z</mi><mo stretchy="false" form="postfix">)</mo><mi>d</mi><mi>z</mi></mrow><annotation encoding="application/x-tex"> E[f(z)] = \int_{-\infty}^{+\infty} \frac{1}{\sigma \sqrt{2\pi}} \exp \left( -\frac{(z-\mu)^2}{2\sigma^2} \right) f(z) dz </annotation></semantics></math></li>
</ul>
</div>
<div id="gauss-hermite-expectation-of-a-normally-distributed-variable-1" class="slide section level1">
<h1>Gauss-Hermite: Expectation of a Normally Distributed Variable</h1>
<ul>
<li>The rule is defined for <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>x</mi><annotation encoding="application/x-tex">x</annotation></semantics></math> however. We need to transform <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>z</mi><annotation encoding="application/x-tex">z</annotation></semantics></math>: <math display="block" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>x</mi><mo>=</mo><mfrac><mrow><mo stretchy="false" form="prefix">(</mo><mi>z</mi><mo>‚àí</mo><mi>Œº</mi><msup><mo stretchy="false" form="postfix">)</mo><mn>2</mn></msup></mrow><mrow><mn>2</mn><msup><mi>œÉ</mi><mn>2</mn></msup></mrow></mfrac><mo accent="false">‚áí</mo><mi>z</mi><mo>=</mo><msqrt><mn>2</mn></msqrt><mi>œÉ</mi><mi>x</mi><mo>+</mo><mi>Œº</mi></mrow><annotation encoding="application/x-tex"> x = \frac{(z-\mu)^2}{2\sigma^2} \Rightarrow z = \sqrt{2} \sigma x + \mu </annotation></semantics></math></li>
<li>This gives us now (just plug in for <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>z</mi><annotation encoding="application/x-tex">z</annotation></semantics></math>) <math display="block" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>E</mi><mo stretchy="false" form="prefix">[</mo><mi>f</mi><mo stretchy="false" form="prefix">(</mo><mi>z</mi><mo stretchy="false" form="postfix">)</mo><mo stretchy="false" form="postfix">]</mo><mo>=</mo><msubsup><mo>‚à´</mo><mrow><mo>‚àí</mo><mi>‚àû</mi></mrow><mrow><mo>+</mo><mi>‚àû</mi></mrow></msubsup><mfrac><mn>1</mn><msqrt><mi>œÄ</mi></msqrt></mfrac><mo>exp</mo><mrow><mo stretchy="true" form="prefix">(</mo><mo>‚àí</mo><msup><mi>x</mi><mn>2</mn></msup><mo stretchy="true" form="postfix">)</mo></mrow><mi>f</mi><mo stretchy="false" form="prefix">(</mo><msqrt><mn>2</mn></msqrt><mi>œÉ</mi><mi>x</mi><mo>+</mo><mi>Œº</mi><mo stretchy="false" form="postfix">)</mo><mi>d</mi><mi>x</mi></mrow><annotation encoding="application/x-tex"> E[f(z)] = \int_{-\infty}^{+\infty} \frac{1}{ \sqrt{\pi}} \exp \left( -x^2 \right) f(\sqrt{2} \sigma x + \mu) dx </annotation></semantics></math></li>
<li>And thus, our approximation to this, using weights <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><msub><mi>œâ</mi><mi>i</mi></msub><annotation encoding="application/x-tex">\omega_i</annotation></semantics></math> and nodes <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><msub><mi>x</mi><mi>i</mi></msub><annotation encoding="application/x-tex">x_i</annotation></semantics></math> is <math display="block" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>E</mi><mo stretchy="false" form="prefix">[</mo><mi>f</mi><mo stretchy="false" form="prefix">(</mo><mi>z</mi><mo stretchy="false" form="postfix">)</mo><mo stretchy="false" form="postfix">]</mo><mo>‚âà</mo><munderover><mo>‚àë</mo><mrow><mi>j</mi><mo>=</mo><mn>1</mn></mrow><mi>J</mi></munderover><mfrac><mn>1</mn><msqrt><mi>œÄ</mi></msqrt></mfrac><msub><mi>œâ</mi><mi>j</mi></msub><mi>f</mi><mo stretchy="false" form="prefix">(</mo><msqrt><mn>2</mn></msqrt><mi>œÉ</mi><msub><mi>x</mi><mi>j</mi></msub><mo>+</mo><mi>Œº</mi><mo stretchy="false" form="postfix">)</mo></mrow><annotation encoding="application/x-tex"> E[f(z)] \approx \sum_{j=1}^J \frac{1}{\sqrt{\pi}} \omega_j f(\sqrt{2} \sigma x_j + \mu)</annotation></semantics></math></li>
</ul>
</div>
<div id="using-quadrature-in-julia" class="slide section level1">
<h1>¬†Using Quadrature in Julia</h1>
<ul>
<li><a href="https://github.com/ajt60gaibb/FastGaussQuadrature.jl" class="uri">https://github.com/ajt60gaibb/FastGaussQuadrature.jl</a></li>
</ul>
<div class="sourceCode"><pre class="sourceCode julia"><code class="sourceCode julia">Pkg.add(<span class="st">&quot;FastGaussQuadrature&quot;</span>)

using FastGaussQuadrature

np = <span class="fl">3</span>

rules = <span class="dt">Dict</span>(<span class="st">&quot;hermite&quot;</span> =&gt; gausshermite(np),
             <span class="st">&quot;chebyshev&quot;</span> =&gt; gausschebyshev(np),
             <span class="st">&quot;legendre&quot;</span> =&gt; gausslegendre(np),
             <span class="st">&quot;lobatto&quot;</span> =&gt; gausslobatto(np))


using DataFrames

nodes = DataFrame([x[<span class="fl">1</span>] <span class="kw">for</span> x <span class="kw">in</span> values(rules)],<span class="dt">Symbol</span>[symbol(x) <span class="kw">for</span> x <span class="kw">in</span> keys(rules)])
weights = DataFrame([x[<span class="fl">2</span>] <span class="kw">for</span> x <span class="kw">in</span> values(rules)],<span class="dt">Symbol</span>[symbol(x) <span class="kw">for</span> x <span class="kw">in</span> keys(rules)])</code></pre></div>
</div>
<div id="quadrature-in-more-dimensions-product-rule" class="slide section level1">
<h1>¬†Quadrature in more dimensions: Product Rule</h1>
<ul>
<li>If we have <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>N</mi><mo>&gt;</mo><mn>1</mn></mrow><annotation encoding="application/x-tex">N&gt;1</annotation></semantics></math>, we can use the product rule: this just takes the kronecker product of all univariate rules.</li>
<li>This works well as long as <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>N</mi><annotation encoding="application/x-tex">N</annotation></semantics></math> is not too large. The number of required function evaluations grows exponentially. <math display="block" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>E</mi><mo stretchy="false" form="prefix">[</mo><mi>G</mi><mo stretchy="false" form="prefix">(</mo><mi>œµ</mi><mo stretchy="false" form="postfix">)</mo><mo stretchy="false" form="postfix">]</mo><mo>=</mo><msub><mo>‚à´</mo><msup><mstyle mathvariant="double-struck"><mi>‚Ñù</mi></mstyle><mi>N</mi></msup></msub><mi>G</mi><mo stretchy="false" form="prefix">(</mo><mi>œµ</mi><mo stretchy="false" form="postfix">)</mo><mi>w</mi><mo stretchy="false" form="prefix">(</mo><mi>œµ</mi><mo stretchy="false" form="postfix">)</mo><mi>d</mi><mi>œµ</mi><mo>‚âà</mo><munderover><mo>‚àë</mo><mrow><msub><mi>j</mi><mn>1</mn></msub><mo>=</mo><mn>1</mn></mrow><msub><mi>J</mi><mn>1</mn></msub></munderover><mi>‚ãØ</mi><munderover><mo>‚àë</mo><mrow><msub><mi>j</mi><mi>N</mi></msub><mo>=</mo><mn>1</mn></mrow><msub><mi>J</mi><mi>N</mi></msub></munderover><msubsup><mi>œâ</mi><msub><mi>j</mi><mn>1</mn></msub><mn>1</mn></msubsup><mi>‚ãØ</mi><msubsup><mi>œâ</mi><msub><mi>j</mi><mi>N</mi></msub><mi>N</mi></msubsup><mi>G</mi><mo stretchy="false" form="prefix">(</mo><msubsup><mi>œµ</mi><msub><mi>j</mi><mn>1</mn></msub><mn>1</mn></msubsup><mo>,</mo><mi>‚Ä¶</mi><mo>,</mo><msubsup><mi>œµ</mi><msub><mi>j</mi><mi>N</mi></msub><mi>N</mi></msubsup><mo stretchy="false" form="postfix">)</mo></mrow><annotation encoding="application/x-tex"> E[G(\epsilon)] = \int_{\mathbb{R}^N} G(\epsilon) w(\epsilon) d\epsilon \approx \sum_{j_1=1}^{J_1} \cdots \sum_{j_N=1}^{J_N} \omega_{j_1}^1 \cdots \omega_{j_N}^N G(\epsilon_{j_1}^1,\dots,\epsilon_{j_N}^N) </annotation></semantics></math> where <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><msubsup><mi>œâ</mi><msub><mi>j</mi><mn>1</mn></msub><mn>1</mn></msubsup><annotation encoding="application/x-tex">\omega_{j_1}^1</annotation></semantics></math> stands for weight index <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><msub><mi>j</mi><mn>1</mn></msub><annotation encoding="application/x-tex">j_1</annotation></semantics></math> in dimension 1, same for <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>œµ</mi><annotation encoding="application/x-tex">\epsilon</annotation></semantics></math>.</li>
<li>Total number of nodes: <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>J</mi><mo>=</mo><msub><mi>J</mi><mn>1</mn></msub><msub><mi>J</mi><mn>2</mn></msub><mi>‚ãØ</mi><msub><mi>J</mi><mi>N</mi></msub></mrow><annotation encoding="application/x-tex">J=J_1 J_2 \cdots J_N</annotation></semantics></math>, and <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><msub><mi>J</mi><mi>i</mi></msub><annotation encoding="application/x-tex">J_i</annotation></semantics></math> can differ from <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><msub><mi>J</mi><mi>k</mi></msub><annotation encoding="application/x-tex">J_k</annotation></semantics></math>.</li>
</ul>
<h2 id="example-for-n3">Example for <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>N</mi><mo>=</mo><mn>3</mn></mrow><annotation encoding="application/x-tex">N=3</annotation></semantics></math></h2>
<ul>
<li>Suppose we have <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msup><mi>œµ</mi><mi>i</mi></msup><mo>‚àº</mo><mi>N</mi><mo stretchy="false" form="prefix">(</mo><mn>0</mn><mo>,</mo><mn>1</mn><mo stretchy="false" form="postfix">)</mo><mo>,</mo><mi>i</mi><mo>=</mo><mn>1</mn><mo>,</mo><mn>2</mn><mo>,</mo><mn>3</mn></mrow><annotation encoding="application/x-tex">\epsilon^i \sim N(0,1),i=1,2,3</annotation></semantics></math> as three uncorrelated random variables.</li>
<li>Let's take <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>J</mi><mo>=</mo><mn>3</mn></mrow><annotation encoding="application/x-tex">J=3</annotation></semantics></math> points in all dimensions, so that in total we have <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msup><mi>J</mi><mi>N</mi></msup><mo>=</mo><mn>27</mn></mrow><annotation encoding="application/x-tex">J^N=27</annotation></semantics></math> points.</li>
<li>We have the nodes and weights from before in <code>rules[&quot;hermite&quot;]</code>.</li>
</ul>
<div class="sourceCode"><pre class="sourceCode julia"><code class="sourceCode julia">nodes = <span class="dt">Any</span>[]
push!(nodes,repeat(rules[<span class="st">&quot;hermite&quot;</span>][<span class="fl">1</span>],inner=[<span class="fl">1</span>],outer=[<span class="fl">9</span>]))
push!(nodes,repeat(rules[<span class="st">&quot;hermite&quot;</span>][<span class="fl">1</span>],inner=[<span class="fl">3</span>],outer=[<span class="fl">3</span>]))
push!(nodes,repeat(rules[<span class="st">&quot;hermite&quot;</span>][<span class="fl">1</span>],inner=[<span class="fl">9</span>],outer=[<span class="fl">1</span>]))
weights = kron(rules[<span class="st">&quot;hermite&quot;</span>][<span class="fl">2</span>],kron(rules[<span class="st">&quot;hermite&quot;</span>][<span class="fl">2</span>],rules[<span class="st">&quot;hermite&quot;</span>][<span class="fl">2</span>]))
df = hcat(DataFrame(weights=weights),DataFrame(nodes,[:dim1,:dim2,:dim3]))</code></pre></div>
<ul>
<li>Imagine you had a function <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>g</mi><annotation encoding="application/x-tex">g</annotation></semantics></math> defined on those 3 dims: in order to approximate the integral, you would have to evaluate <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>g</mi><annotation encoding="application/x-tex">g</annotation></semantics></math> at all combinations of <code>dimx</code>, multiply with the corresponding weight, and sum.</li>
</ul>
</div>
<div id="alternatives-to-the-product-rule" class="slide section level1">
<h1>Alternatives to the Product Rule</h1>
<ul>
<li>Monomial Rules: They grow only linearly.</li>
<li>Please refer to <span class="citation">(Judd 1998)</span> for more details.</li>
</ul>
</div>
<div id="monte-carlo-integration" class="slide section level1">
<h1>Monte Carlo Integration</h1>
<ul>
<li>A widely used method is to just draw <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>N</mi><annotation encoding="application/x-tex">N</annotation></semantics></math> points randomly from the space of the shock <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>œµ</mi><annotation encoding="application/x-tex">\epsilon</annotation></semantics></math>, and to assign equal weights <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>œâ</mi><mi>j</mi></msub><mo>=</mo><mfrac><mn>1</mn><mi>N</mi></mfrac></mrow><annotation encoding="application/x-tex">\omega_j=\frac{1}{N}</annotation></semantics></math> to all of them.</li>
<li>The expectation is then <math display="block" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>E</mi><mo stretchy="false" form="prefix">[</mo><mi>G</mi><mo stretchy="false" form="prefix">(</mo><mi>œµ</mi><mo stretchy="false" form="postfix">)</mo><mo stretchy="false" form="postfix">]</mo><mo>‚âà</mo><mfrac><mn>1</mn><mi>N</mi></mfrac><munderover><mo>‚àë</mo><mrow><mi>j</mi><mo>=</mo><mn>1</mn></mrow><mi>N</mi></munderover><mi>G</mi><mo stretchy="false" form="prefix">(</mo><msub><mi>œµ</mi><mi>j</mi></msub><mo stretchy="false" form="postfix">)</mo></mrow><annotation encoding="application/x-tex"> E[G(\epsilon)] \approx \frac{1}{N} \sum_{j=1}^N  G(\epsilon_j) </annotation></semantics></math></li>
<li>This in general a very inefficient method.</li>
<li>Particularly in more than 1 dimensions, the number of points needed for good accuracy is very large.</li>
</ul>
<h2 id="quasi-monte-carlo-integration">Quasi Monte Carlo Integration</h2>
<ul>
<li>Uses non-product techniques to construct a grid of uniformly spaced points.</li>
<li>The researcher controlls the number of points.</li>
<li>We need to construct equidistributed points.</li>
<li>Typically one uses a low-discrepancy sequence of points, e.g. the Weyl sequence:</li>
<li><math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>x</mi><mi>n</mi></msub><mo>=</mo><mrow><mi>n</mi><mi>v</mi></mrow></mrow><annotation encoding="application/x-tex">x_n = {n v}</annotation></semantics></math> where <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>v</mi><annotation encoding="application/x-tex">v</annotation></semantics></math> is an irrational number and <code>{}</code> stands for the fractional part of a number. for <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>v</mi><mo>=</mo><msqrt><mn>2</mn></msqrt></mrow><annotation encoding="application/x-tex">v=\sqrt{2}</annotation></semantics></math>, <math display="block" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>x</mi><mn>1</mn></msub><mo>=</mo><mo stretchy="false" form="prefix">{</mo><mn>1</mn><msqrt><mn>2</mn></msqrt><mo stretchy="false" form="postfix">}</mo><mo>=</mo><mo stretchy="false" form="prefix">{</mo><mn>1.4142</mn><mo stretchy="false" form="postfix">}</mo><mo>=</mo><mn>0.4142</mn><mo>,</mo><msub><mi>x</mi><mn>2</mn></msub><mo>=</mo><mo stretchy="false" form="prefix">{</mo><mn>2</mn><msqrt><mn>2</mn></msqrt><mo stretchy="false" form="postfix">}</mo><mo>=</mo><mo stretchy="false" form="prefix">{</mo><mn>2.8242</mn><mo stretchy="false" form="postfix">}</mo><mo>=</mo><mn>0.8242</mn><mo>,</mo><mi>.</mi><mi>.</mi><mi>.</mi></mrow><annotation encoding="application/x-tex"> x_1 = \{1 \sqrt{2}\} = \{1.4142\} = 0.4142, x_2 = \{2 \sqrt{2}\} = \{2.8242\} = 0.8242,... </annotation></semantics></math></li>
<li>Other low-discrepancy sequences are Niederreiter, Haber, Baker or Sobol.</li>
</ul>
<div class="sourceCode"><pre class="sourceCode julia"><code class="sourceCode julia">Pkg.add(<span class="st">&quot;Sobol&quot;</span>)
using Sobol
using PyPlot
s = SobolSeq(<span class="fl">2</span>)
p = hcat([next(s) <span class="kw">for</span> i = <span class="fl">1</span>:<span class="fl">1024</span>]...)&#39;
subplot(<span class="fl">111</span>, aspect=<span class="st">&quot;equal&quot;</span>)
plot(p[:,<span class="fl">1</span>], p[:,<span class="fl">2</span>], <span class="st">&quot;r.&quot;</span>)</code></pre></div>
<div class="figure">
<img src="figs/Sobol.png" alt="Sobol Sequences in [0,1]^2" />
<p class="caption">Sobol Sequences in [0,1]^2</p>
</div>
</div>
<div id="correlated-shocks" class="slide section level1">
<h1>Correlated Shocks</h1>
<ul>
<li>We often face situations where the shocks are in fact correlated.
<ul>
<li>One very typical case is an AR1 process: <math display="block" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>z</mi><mrow><mi>t</mi><mo>+</mo><mn>1</mn></mrow></msub><mo>=</mo><mi>œÅ</mi><msub><mi>z</mi><mi>t</mi></msub><mo>+</mo><msub><mi>Œµ</mi><mi>t</mi></msub><mo>,</mo><mi>Œµ</mi><mo>‚àº</mo><mi>N</mi><mo stretchy="false" form="prefix">(</mo><mn>0</mn><mo>,</mo><msup><mi>œÉ</mi><mn>2</mn></msup><mo stretchy="false" form="postfix">)</mo></mrow><annotation encoding="application/x-tex"> z_{t+1} = \rho z_t + \varepsilon_t, \varepsilon \sim N(0,\sigma^2) </annotation></semantics></math></li>
</ul></li>
<li>The general case is again: <math display="block" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>E</mi><mo stretchy="false" form="prefix">[</mo><mi>G</mi><mo stretchy="false" form="prefix">(</mo><mi>œµ</mi><mo stretchy="false" form="postfix">)</mo><mo stretchy="false" form="postfix">]</mo><mo>=</mo><msub><mo>‚à´</mo><msup><mstyle mathvariant="double-struck"><mi>‚Ñù</mi></mstyle><mi>N</mi></msup></msub><mi>G</mi><mo stretchy="false" form="prefix">(</mo><mi>œµ</mi><mo stretchy="false" form="postfix">)</mo><mi>w</mi><mo stretchy="false" form="prefix">(</mo><mi>œµ</mi><mo stretchy="false" form="postfix">)</mo><mi>d</mi><mi>œµ</mi><mo>‚âà</mo><munderover><mo>‚àë</mo><mrow><msub><mi>j</mi><mn>1</mn></msub><mo>=</mo><mn>1</mn></mrow><msub><mi>J</mi><mn>1</mn></msub></munderover><mi>‚ãØ</mi><munderover><mo>‚àë</mo><mrow><msub><mi>j</mi><mi>N</mi></msub><mo>=</mo><mn>1</mn></mrow><msub><mi>J</mi><mi>N</mi></msub></munderover><msubsup><mi>œâ</mi><msub><mi>j</mi><mn>1</mn></msub><mn>1</mn></msubsup><mi>‚ãØ</mi><msubsup><mi>œâ</mi><msub><mi>j</mi><mi>N</mi></msub><mi>N</mi></msubsup><mi>G</mi><mo stretchy="false" form="prefix">(</mo><msubsup><mi>œµ</mi><msub><mi>j</mi><mn>1</mn></msub><mn>1</mn></msubsup><mo>,</mo><mi>‚Ä¶</mi><mo>,</mo><msubsup><mi>œµ</mi><msub><mi>j</mi><mi>N</mi></msub><mi>N</mi></msubsup><mo stretchy="false" form="postfix">)</mo></mrow><annotation encoding="application/x-tex"> E[G(\epsilon)] = \int_{\mathbb{R}^N} G(\epsilon) w(\epsilon) d\epsilon \approx \sum_{j_1=1}^{J_1} \cdots \sum_{j_N=1}^{J_N} \omega_{j_1}^1 \cdots \omega_{j_N}^N G(\epsilon_{j_1}^1,\dots,\epsilon_{j_N}^N) </annotation></semantics></math></li>
<li>Now <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>œµ</mi><mo>‚àº</mo><mi>N</mi><mo stretchy="false" form="prefix">(</mo><mi>Œº</mi><mo>,</mo><mi>Œ£</mi><mo stretchy="false" form="postfix">)</mo></mrow><annotation encoding="application/x-tex">\epsilon \sim N(\mu,\Sigma)</annotation></semantics></math> where <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>Œ£</mi><annotation encoding="application/x-tex">\Sigma</annotation></semantics></math> is an N by N variance-covariance matrix.</li>
<li>The multivariate density is <math display="block" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>w</mi><mo stretchy="false" form="prefix">(</mo><mi>œµ</mi><mo stretchy="false" form="postfix">)</mo><mo>=</mo><mo stretchy="false" form="prefix">(</mo><mn>2</mn><mi>œÄ</mi><msup><mo stretchy="false" form="postfix">)</mo><mrow><mo>‚àí</mo><mi>N</mi><mi>/</mi><mn>2</mn></mrow></msup><mi>d</mi><mi>e</mi><mi>t</mi><mo stretchy="false" form="prefix">(</mo><mi>Œ£</mi><msup><mo stretchy="false" form="postfix">)</mo><mrow><mo>‚àí</mo><mn>1</mn><mi>/</mi><mn>2</mn></mrow></msup><mo>exp</mo><mrow><mo stretchy="true" form="prefix">(</mo><mo>‚àí</mo><mfrac><mn>1</mn><mn>2</mn></mfrac><mo stretchy="false" form="prefix">(</mo><mi>œµ</mi><mo>‚àí</mo><mi>Œº</mi><msup><mo stretchy="false" form="postfix">)</mo><mi>T</mi></msup><mo stretchy="false" form="prefix">(</mo><mi>œµ</mi><mo>‚àí</mo><mi>Œº</mi><mo stretchy="false" form="postfix">)</mo><mo stretchy="true" form="postfix">)</mo></mrow></mrow><annotation encoding="application/x-tex">w(\epsilon) = (2\pi)^{-N/2} det(\Sigma)^{-1/2} \exp \left(-\frac{1}{2}(\epsilon - \mu)^T (\epsilon - \mu) \right)</annotation></semantics></math></li>
<li>We need to perform a change of variables before we can integrate this.</li>
<li>Given <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>Œ£</mi><annotation encoding="application/x-tex">\Sigma</annotation></semantics></math> is symmetric and positive semi-definite, it has a Cholesky decomposition, <math display="block" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>Œ£</mi><mo>=</mo><mi>Œ©</mi><msup><mi>Œ©</mi><mi>T</mi></msup></mrow><annotation encoding="application/x-tex"> \Sigma = \Omega \Omega^T </annotation></semantics></math> where <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>Œ©</mi><annotation encoding="application/x-tex">\Omega</annotation></semantics></math> is a lower-triangular with strictly positive entries.</li>
<li>The linear change of variables is then <math display="block" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>v</mi><mo>=</mo><msup><mi>Œ©</mi><mrow><mo>‚àí</mo><mn>1</mn></mrow></msup><mo stretchy="false" form="prefix">(</mo><mi>œµ</mi><mo>‚àí</mo><mi>Œº</mi><mo stretchy="false" form="postfix">)</mo></mrow><annotation encoding="application/x-tex"> v = \Omega^{-1} (\epsilon - \mu)  </annotation></semantics></math></li>
<li>Plugging this in gives <math display="block" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><munderover><mo>‚àë</mo><mrow><mi>j</mi><mo>=</mo><mn>1</mn></mrow><mi>J</mi></munderover><msub><mi>œâ</mi><mi>j</mi></msub><mi>G</mi><mo stretchy="false" form="prefix">(</mo><mi>Œ©</mi><msub><mi>v</mi><mi>j</mi></msub><mo>+</mo><mi>Œº</mi><mo stretchy="false" form="postfix">)</mo><mo>‚â°</mo><munderover><mo>‚àë</mo><mrow><mi>j</mi><mo>=</mo><mn>1</mn></mrow><mi>J</mi></munderover><msub><mi>œâ</mi><mi>j</mi></msub><mi>G</mi><mo stretchy="false" form="prefix">(</mo><msub><mi>œµ</mi><mi>j</mi></msub><mo stretchy="false" form="postfix">)</mo></mrow><annotation encoding="application/x-tex"> \sum_{j=1}^J \omega_j  G(\Omega v_j + \mu) \equiv \sum_{j=1}^J \omega_j  G(\epsilon_j) </annotation></semantics></math> where <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>v</mi><mo>‚àº</mo><mi>N</mi><mo stretchy="false" form="prefix">(</mo><mn>0</mn><mo>,</mo><msub><mi>I</mi><mi>N</mi></msub><mo stretchy="false" form="postfix">)</mo></mrow><annotation encoding="application/x-tex">v\sim N(0,I_N)</annotation></semantics></math>.</li>
<li>So, we can follow the exact same steps as with the uncorrelated shocks, but need to adapt the nodes.</li>
</ul>
</div>
<div id="references" class="slide section level1">
<h1>References</h1>
<ul>
<li>The Integration part of these slides are based on <span class="citation">(L. Maliar and Maliar 2013)</span> chapter 5</li>
</ul>
<div id="refs" class="references">
<div id="ref-judd-book">
<p>Judd, K. L. (1998). <em>Numerical methods in economics</em>. The MIT Press.</p>
</div>
<div id="ref-maliar-maliar">
<p>Maliar, L., &amp; Maliar, S. (2013). Numerical methods for large scale dynamic economic models. <em>Handbook of Computational Economics</em>, <em>3</em>, 325. doi:<a href="https://doi.org/http://dx.doi.org/10.1016/B978-0-444-52980-0.00007-4">http://dx.doi.org/10.1016/B978-0-444-52980-0.00007-4</a></p>
</div>
</div>
</div>
</body>
</html>
