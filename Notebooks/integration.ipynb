{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "% Computational Economics: Numerical Integrals and Derivatives\n",
    "% Florian Oswald\n",
    "% Sciences Po, 2017\n",
    "\n",
    "\n",
    "# Numerical Differentiation and Integration\n",
    "\n",
    "## Derivatives\n",
    "\n",
    "1. Finite Differencing: a numerical approximation\n",
    "\t* Based on Taylor's Theorem\n",
    "\t* Observe variation in function values from evaluating it at \"close\" points.\n",
    "\t* Forward Differencing and Central Differencing\n",
    "2. Automatic Differentiation\n",
    "\t* Breaks down the actual `code` that defines a function and performs elementary differentiation rules, after disecting expressions via the chain rule.\n",
    "\t* This produces **analytic** derivatives, i.e. there is **no** approximation error.\n",
    "\t* This is the future.\n",
    "3. Symbolic Differentiation\n",
    "\t* Some languages (most notably Mathematica) support symbolic algebra. Very useful sometimes if one needs to work through complicated expressions.\n",
    "\t* Not very useful for high computational demands, i.e. repeated computation of derivatives in an optimization routine.\n",
    "\n",
    "-------------\n",
    "\n",
    "# Finite Differences\n",
    "\n",
    "* Consider the definition of the derivative of $f$ at point $x$:\n",
    "\t$$ f'(x) = \\lim_{h\\to0}\\frac{f(x+h)-f(x)}{h} $$\n",
    "* The simplest way to calculate a numerical derivative is to replicate this computation for small $h$ with:\n",
    "\t$$ f'(x) \\approx \\frac{f(x+h)-f(x)}{h},\\quad h\\text{ small.} $$\n",
    "* This is known as the Forward Difference approach.\n",
    "* There are different approaches, e.g. the central difference approach does\n",
    "\t$$ f'(x) \\approx \\frac{f(x+h)-f(x-h)}{2h},\\quad h\\text{ small.} $$\n",
    "* How does this perform?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "attributes": {
     "classes": [
      "julia"
     ],
     "id": ""
    }
   },
   "outputs": [],
   "source": [
    "using Gadfly\n",
    "f(x) = 2 - x^2\n",
    "c = -0.75\n",
    "sec_line(h) = x -> f(c) + (f(c + h) - f(c))/h * (x - c)\n",
    "plot([f, sec_line(1), sec_line(.5), sec_line(.25), sec_line(.05)], -1, 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* What's the problem? Well, what is *small*?\n",
    "\n",
    "\n",
    "-------------\n",
    "\n",
    "# Finite Differences: what's the right step size $h$?\n",
    "\n",
    "* Theoretically, we would like to have $h$ as small as possible, since we want to approximate the limit at zero.\n",
    "* In practice, on a computer, there is a limit to this. There is a smallest representable number, as we know.\n",
    "* `eps()`.\n",
    "* One can show that the optimal step size is $h=\\sqrt{\\texttt{eps()}}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Automatic Differentiation (AD)\n",
    "\n",
    "* 2 modes: Forward and Reverse Mode.\n",
    "* The basic idea is that the derivative of any function can be decomposed into some basic algebraic operations.\n",
    "* The [wikipedia page is informative](https://en.wikipedia.org/wiki/Automatic_differentiation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "![[By Berland at en.wikipedia [Public domain], from Wikimedia Commons](https://commons.wikimedia.org/wiki/File%3AAutomaticDifferentiation.png)](figs/wikipedia-AD.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example\n",
    "\n",
    "* Suppose we want to differentiate $f(x_1,x_2) = x_1 x_2 + \\sin x_1$\n",
    "* We label subexpressions by $w_i$ as follows:\n",
    "\t$$ \\begin{array}{cl}\n",
    "\tf(x_1,x_2) &= x_1 x_2 + \\sin x_1 \\\\\n",
    "\t&= w_1 w_2 + sin w_1 \\\\\n",
    "\t&= w_3  + w_4 \\\\\n",
    "\t&= w_5 \n",
    "\t\\end{array} \n",
    "\t$$\n",
    "* Computation of the partial derivative starts with the seed value, i.e. $\\dot{w}_1 = \\frac{\\partial x_1}{\\partial x_1} = 1$.\n",
    "* We store for each subexpression both the value and the derivative, i.e. $(w_i,\\dot{w}_i)$\n",
    "* We then sweep through the expression tree as in this picture:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "![[By Berland at en.wikipedia [Public domain], from Wikimedia Commons](https://commons.wikimedia.org/wiki/File%3AForwardAccumulationAutomaticDifferentiation.png)](figs/wikipedia-AD-example.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-----------------\n",
    "\n",
    "# AD in Julia\n",
    "\n",
    "* The organisation here is [http://www.juliadiff.org](http://www.juliadiff.org)\n",
    "* There are many packages to perform differentiation with Julia here.\n",
    "* Many packages rely on the machinery here. \n",
    "* Let's quickly look at [https://github.com/JuliaDiff/ForwardDiff.jl](https://github.com/JuliaDiff/ForwardDiff.jl)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "attributes": {
     "classes": [
      "julia"
     ],
     "id": ""
    }
   },
   "outputs": [],
   "source": [
    "# from ForwardDiff's readme:\n",
    "using ForwardDiff\n",
    "f(x::Vector) = sum(sin, x) + prod(tan, x) * sum(sqrt, x)\n",
    "x = rand(5); # get 5 random points\n",
    "g = ForwardDiff.gradient(f); # g = ∇f\n",
    "j = ForwardDiff.jacobian(g); # j = J(∇f)\n",
    "ForwardDiff.hessian(f, x) # H(f)(x) == J(∇f)(x), as expected"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* The authors provide some benchmarks. Let's run those:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "attributes": {
     "classes": [
      "julia"
     ],
     "id": ""
    }
   },
   "outputs": [],
   "source": [
    "include(joinpath(Pkg.dir(\"ForwardDiff\"),\"benchmarks\",\"run_all_benchmarks(ForwardDiffBenchmarks.Rosenbrock)\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----------------\n",
    "\n",
    "\n",
    "# Numerical Approximation of Integrals\n",
    "\n",
    "* We will focus on methods that represent integrals as weighted sums.\n",
    "* The typical representation will look like:\n",
    "\t$$ E[G(\\epsilon)] = \\int_{\\mathbb{R}^N} G(\\epsilon) w(\\epsilon) d\\epsilon \\approx \\sum_{j=1}^J \\omega_j G(\\epsilon_j) $$\n",
    "\t* $N$ is the dimensionality of the integration problem.\n",
    "\t* $G:\\mathbb{R}^N \\mapsto \\mathbb{R}$ is the function we want to integrate wrt $\\epsilon \\in \\mathbb{R}^N$.\n",
    "\t* $w$ is a density function s.t. $\\int_{\\mathbb{R}^n} w(\\epsilon) d\\epsilon = 1$.\n",
    "\t* $\\omega$ are weights such that (most of the time) $\\sum_{j=1}^J \\omega_j = 1$.\n",
    "<!-- * We will look at normal shocks $\\epsilon \\sim N(0_N,I_N)$\n",
    "\t* in that case, $w(\\epsilon) = (2\\pi)^{-N/2} \\exp \\left(-\\frac{1}{2}\\epsilon^T \\epsilon \\right)$\n",
    "\t* $I_N$ is the n by n identity matrix, i.e. there is no correlation among the shocks for now.\n",
    "\t* Other random processes will require different weighting functions, but the principle is identical.\n",
    " -->\n",
    " * For now, let's say that $N=1$\n",
    "\n",
    "-----------------\n",
    "\n",
    "\n",
    "# Quadrature Rules\n",
    "\n",
    "* We focus exclusively on those and leave Simpson and Newton Cowtes formulas out.\n",
    "\t* This is because Quadrature is the method that in many situations gives highes accuracy with lowest computational cost.\n",
    "* Quadrature provides a rule to compute weights $w_j$ and nodes $\\epsilon_j$.\n",
    "* There are many different quadrature rules.\n",
    "* They differ in their domain and weighting function.\n",
    "* [https://en.wikipedia.org/wiki/Gaussian_quadrature](https://en.wikipedia.org/wiki/Gaussian_quadrature)\n",
    "* In general, we can convert our function domain to a rule-specific domain with change of variables.\n",
    "\n",
    "\n",
    "-----------------\n",
    "\n",
    "# Gauss-Hermite: Expectation of a Normally Distributed Variable\n",
    "\n",
    "* There are many different rules, all specific to a certain random process.\n",
    "* Gauss-Hermite is designed for an integral of the form\n",
    "\t$$ \\int_{-\\infty}^{+\\infty} e^{-x^2} G(x) dx $$\n",
    "\tand where we would approximate \n",
    "\t$$ \\int_{-\\infty}^{+\\infty} e^{-x^2} f(x) dx \\approx \\sum_{i=1}^n \\omega_i G(x_i) $$\n",
    "* Now, let's say we want to approximate the expected value of function $f$ when it's argument $z\\sim N(\\mu,\\sigma^2)$:\n",
    "\t$$ E[f(z)] = \\int_{-\\infty}^{+\\infty} \\frac{1}{\\sigma \\sqrt{2\\pi}} \\exp \\left( -\\frac{(z-\\mu)^2}{2\\sigma^2} \\right) f(z) dz $$\n",
    "\n",
    "--------------\n",
    "\n",
    "# Gauss-Hermite: Expectation of a Normally Distributed Variable\n",
    "\n",
    "* The rule is defined for $x$ however. We need to transform $z$:\n",
    "\t$$ x = \\frac{(z-\\mu)^2}{2\\sigma^2} \\Rightarrow z = \\sqrt{2} \\sigma x + \\mu $$\n",
    "* This gives us now (just plug in for $z$)\n",
    "\t$$ E[f(z)] = \\int_{-\\infty}^{+\\infty} \\frac{1}{ \\sqrt{\\pi}} \\exp \\left( -x^2 \\right) f(\\sqrt{2} \\sigma x + \\mu) dx $$\n",
    "* And thus, our approximation to this, using weights $\\omega_i$ and nodes $x_i$ is\n",
    "\t$$ E[f(z)] \\approx \\sum_{j=1}^J \\frac{1}{\\sqrt{\\pi}} \\omega_j f(\\sqrt{2} \\sigma x_j + \\mu)$$\n",
    "\n",
    "------------\n",
    "\n",
    "# Using Quadrature in Julia\n",
    "\n",
    "* [https://github.com/ajt60gaibb/FastGaussQuadrature.jl](https://github.com/ajt60gaibb/FastGaussQuadrature.jl)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "attributes": {
     "classes": [
      "julia"
     ],
     "id": ""
    }
   },
   "outputs": [],
   "source": [
    "Pkg.add(\"FastGaussQuadrature\")\n",
    "\n",
    "using FastGaussQuadrature\n",
    "\n",
    "np = 3\n",
    "\n",
    "rules = Dict(\"hermite\" => gausshermite(np),\n",
    "             \"chebyshev\" => gausschebyshev(np),\n",
    "             \"legendre\" => gausslegendre(np),\n",
    "             \"lobatto\" => gausslobatto(np))\n",
    "\n",
    "\n",
    "using DataFrames\n",
    "\n",
    "nodes = DataFrame([x[1] for x in values(rules)],Symbol[symbol(x) for x in keys(rules)])\n",
    "weights = DataFrame([x[2] for x in values(rules)],Symbol[symbol(x) for x in keys(rules)])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "------------\n",
    "\n",
    "\n",
    "\n",
    "# Quadrature in more dimensions: Product Rule\n",
    "\n",
    "* If we have $N>1$, we can use the product rule: this just takes the kronecker product of all univariate rules.\n",
    "* This works well as long as $N$ is not too large. The number of required function evaluations grows exponentially.\n",
    "\t$$ E[G(\\epsilon)] = \\int_{\\mathbb{R}^N} G(\\epsilon) w(\\epsilon) d\\epsilon \\approx \\sum_{j_1=1}^{J_1} \\cdots \\sum_{j_N=1}^{J_N} \\omega_{j_1}^1 \\cdots \\omega_{j_N}^N G(\\epsilon_{j_1}^1,\\dots,\\epsilon_{j_N}^N) $$\n",
    "\twhere $\\omega_{j_1}^1$ stands for weight index $j_1$ in dimension 1, same for $\\epsilon$.\n",
    "* Total number of nodes: $J=J_1 J_2 \\cdots J_N$, and $J_i$ can differ from $J_k$.\n",
    "\n",
    "## Example for $N=3$\n",
    "\n",
    "* Suppose we have $\\epsilon^i \\sim N(0,1),i=1,2,3$ as three uncorrelated random variables.\n",
    "* Let's take $J=3$ points in all dimensions, so that in total we have $J^N=27$ points.\n",
    "* We have the nodes and weights from before in `rules[\"hermite\"]`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "attributes": {
     "classes": [
      "julia"
     ],
     "id": ""
    }
   },
   "outputs": [],
   "source": [
    "nodes = Any[]\n",
    "push!(nodes,repeat(rules[\"hermite\"][1],inner=[1],outer=[9]))\n",
    "push!(nodes,repeat(rules[\"hermite\"][1],inner=[3],outer=[3]))\n",
    "push!(nodes,repeat(rules[\"hermite\"][1],inner=[9],outer=[1]))\n",
    "weights = kron(rules[\"hermite\"][2],kron(rules[\"hermite\"][2],rules[\"hermite\"][2]))\n",
    "df = hcat(DataFrame(weights=weights),DataFrame(nodes,[:dim1,:dim2,:dim3]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Imagine you had a function $g$ defined on those 3 dims: in order to approximate the integral, you would have to evaluate $g$ at all combinations of `dimx`, multiply with the corresponding weight, and sum.\n",
    "\n",
    "\n",
    "----------------\n",
    "\n",
    "# Alternatives to the Product Rule\n",
    "\n",
    "* Monomial Rules: They grow only linearly.\n",
    "* Please refer to [@judd-book] for more details.\n",
    "\n",
    "\n",
    "-----------------\n",
    "\n",
    "# Monte Carlo Integration\n",
    "\n",
    "* A widely used method is to just draw $N$ points randomly from the space of the shock $\\epsilon$, and to assign equal weights $\\omega_j=\\frac{1}{N}$ to all of them.\n",
    "* The expectation is then\n",
    "\t$$ E[G(\\epsilon)] \\approx \\frac{1}{N} \\sum_{j=1}^N  G(\\epsilon_j) $$\n",
    "* This in general a very inefficient method.\n",
    "* Particularly in more than 1 dimensions, the number of points needed for good accuracy is very large.\n",
    "\n",
    "## Quasi Monte Carlo Integration\n",
    "\n",
    "* Uses non-product techniques to construct a grid of uniformly spaced points.\n",
    "* The researcher controlls the number of points. \n",
    "* We need to construct equidistributed points.\n",
    "* Typically one uses a low-discrepancy sequence of points, e.g. the Weyl sequence:\n",
    "* $x_n = {n v}$ where $v$ is an irrational number and `{}` stands for the fractional part of a number. for $v=\\sqrt{2}$,\n",
    "\t$$ x_1 = \\{1 \\sqrt{2}\\} = \\{1.4142\\} = 0.4142, x_2 = \\{2 \\sqrt{2}\\} = \\{2.8242\\} = 0.8242,... $$\n",
    "* Other low-discrepancy sequences are Niederreiter, Haber, Baker or Sobol."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "attributes": {
     "classes": [
      "julia"
     ],
     "id": ""
    }
   },
   "outputs": [],
   "source": [
    "Pkg.add(\"Sobol\")\n",
    "using Sobol\n",
    "using PyPlot\n",
    "s = SobolSeq(2)\n",
    "p = hcat([next(s) for i = 1:1024]...)'\n",
    "subplot(111, aspect=\"equal\")\n",
    "plot(p[:,1], p[:,2], \"r.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Sobol Sequences in [0,1]^2](figs/Sobol.png) \n",
    "\n",
    "----------------\n",
    "\n",
    "# Correlated Shocks\n",
    "\n",
    "* We often face situations where the shocks are in fact correlated.\n",
    "\t* One very typical case is an AR1 process:\n",
    "\t$$ z_{t+1} = \\rho z_t + \\varepsilon_t, \\varepsilon \\sim N(0,\\sigma^2) $$\n",
    "* The general case is again:\n",
    "\t$$ E[G(\\epsilon)] = \\int_{\\mathbb{R}^N} G(\\epsilon) w(\\epsilon) d\\epsilon \\approx \\sum_{j_1=1}^{J_1} \\cdots \\sum_{j_N=1}^{J_N} \\omega_{j_1}^1 \\cdots \\omega_{j_N}^N G(\\epsilon_{j_1}^1,\\dots,\\epsilon_{j_N}^N) $$\n",
    "* Now $\\epsilon \\sim N(\\mu,\\Sigma)$ where $\\Sigma$ is an N by N variance-covariance matrix.\n",
    "* The multivariate density is\n",
    "\t$$w(\\epsilon) = (2\\pi)^{-N/2} det(\\Sigma)^{-1/2} \\exp \\left(-\\frac{1}{2}(\\epsilon - \\mu)^T (\\epsilon - \\mu) \\right)$$\n",
    "* We need to perform a change of variables before we can integrate this.\n",
    "* Given $\\Sigma$ is symmetric and positive semi-definite, it has a Cholesky decomposition, \n",
    "\t$$ \\Sigma = \\Omega \\Omega^T $$\n",
    "\twhere $\\Omega$ is a lower-triangular with strictly positive entries.\n",
    "* The linear change of variables is then\n",
    "\t$$ v = \\Omega^{-1} (\\epsilon - \\mu)  $$\n",
    "* Plugging this in gives\n",
    "\t$$ \\sum_{j=1}^J \\omega_j  G(\\Omega v_j + \\mu) \\equiv \\sum_{j=1}^J \\omega_j  G(\\epsilon_j) $$\n",
    "\twhere $v\\sim N(0,I_N)$.\n",
    "* So, we can follow the exact same steps as with the uncorrelated shocks, but need to adapt the nodes.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# References\n",
    "\n",
    "* The Integration part of these slides are based on [@maliar-maliar] chapter 5"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 0
}
